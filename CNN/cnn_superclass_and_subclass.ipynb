{"cells":[{"cell_type":"code","execution_count":null,"id":"198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3","metadata":{"id":"198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3"},"outputs":[],"source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import torchvision\n","\n","from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n","from torchvision import transforms\n","from PIL import Image"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/NNDL_Project"],"metadata":{"id":"YmReUJVHlqqv"},"id":"YmReUJVHlqqv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","! unzip -o train_shuffle.zip"],"metadata":{"id":"uTRjVHXWlq-P"},"id":"uTRjVHXWlq-P","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","! unzip -o test_shuffle.zip"],"metadata":{"id":"2kkl95U1lsSn"},"id":"2kkl95U1lsSn","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c370d643-46fd-4d03-bb17-a875e79d5e2c","metadata":{"id":"c370d643-46fd-4d03-bb17-a875e79d5e2c"},"outputs":[],"source":["# Create Dataset class for multilabel classification\n","class MultiClassImageDataset(Dataset):\n","    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n","        self.ann_df = ann_df\n","        self.super_map_df = super_map_df\n","        self.sub_map_df = sub_map_df\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.ann_df)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.ann_df['image'][idx]\n","        img_path = os.path.join(self.img_dir, img_name)\n","        image = Image.open(img_path).convert('RGB')\n","\n","        super_idx = self.ann_df['superclass_index'][idx]\n","        super_label = self.super_map_df['class'][super_idx]\n","\n","        sub_idx = self.ann_df['subclass_index'][idx]\n","        sub_label = self.sub_map_df['class'][sub_idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, super_idx, super_label, sub_idx, sub_label\n","\n","class MultiClassImageTestDataset(Dataset):\n","    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n","        self.super_map_df = super_map_df\n","        self.sub_map_df = sub_map_df\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self): # Count files in img_dir\n","        return len([fname for fname in os.listdir(self.img_dir)])\n","\n","    def __getitem__(self, idx):\n","        img_name = str(idx) + '.jpg'\n","        img_path = os.path.join(self.img_dir, img_name)\n","        image = Image.open(img_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, img_name"]},{"cell_type":"code","source":["train_ann_df = pd.read_csv('train_data.csv')\n","super_map_df = pd.read_csv('superclass_mapping.csv')\n","sub_map_df = pd.read_csv('subclass_mapping.csv')\n","\n","train_img_dir = 'train_shuffle'\n","test_img_dir = 'test_shuffle'\n","\n","image_preprocessing = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0), std=(1)),\n","])\n","\n","# Create train and val split\n","train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n","train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n","\n","# Create test dataset\n","test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n","\n","# Create dataloaders\n","batch_size = 64\n","train_loader = DataLoader(train_dataset,\n","                          batch_size=batch_size,\n","                          shuffle=True)\n","\n","val_loader = DataLoader(val_dataset,\n","                        batch_size=batch_size,\n","                        shuffle=True)\n","\n","test_loader = DataLoader(test_dataset,\n","                         batch_size=1,\n","                         shuffle=False)"],"metadata":{"id":"bpeHZTCFllpa"},"id":"bpeHZTCFllpa","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"bf33a131-0c66-40dc-b8d4-ba5d0f840840","metadata":{"id":"bf33a131-0c66-40dc-b8d4-ba5d0f840840"},"outputs":[],"source":["# Simple CNN\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.block1 = nn.Sequential(\n","                        nn.Conv2d(3, 32, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(32),\n","                        nn.Conv2d(32, 32, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(32),\n","                        nn.Conv2d(32, 32, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(32),\n","                        nn.MaxPool2d(2, 2)\n","                      )\n","\n","        self.block2 = nn.Sequential(\n","                        nn.Conv2d(32, 64, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(64),\n","                        nn.Conv2d(64, 64, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(64),\n","                        nn.Conv2d(64, 64, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(64),\n","                        nn.MaxPool2d(2, 2)\n","                      )\n","\n","        self.block3 = nn.Sequential(\n","                        nn.Conv2d(64, 128, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(128),\n","                        nn.Conv2d(128, 128, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(128),\n","                        nn.Conv2d(128, 128, 3, padding='same'),\n","                        nn.ReLU(),\n","                        nn.BatchNorm2d(128),\n","                        nn.MaxPool2d(2, 2)\n","                      )\n","\n","        self.fc1 = nn.Linear(4*4*128, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3a = nn.Linear(128, 4)\n","        self.fc3b = nn.Linear(128, 88)\n","\n","    def forward(self, x):\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        super_out = self.fc3a(x)\n","        sub_out = self.fc3b(x)\n","        return super_out, sub_out\n","\n","class Trainer():\n","    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.test_loader = test_loader\n","        self.device = device\n","\n","    def train_epoch(self):\n","        running_loss = 0.0\n","        device = self.device\n","        for i, data in enumerate(self.train_loader):\n","            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n","\n","            self.optimizer.zero_grad()\n","            super_outputs, sub_outputs = self.model(inputs)\n","            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f'Training loss: {running_loss/i:.3f}')\n","\n","    def validate_epoch(self):\n","        super_correct = 0\n","        sub_correct = 0\n","        total = 0\n","        running_loss = 0.0\n","        device = self.device\n","        with torch.no_grad():\n","            for i, data in enumerate(self.val_loader):\n","                inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n","\n","                super_outputs, sub_outputs = self.model(inputs)\n","                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n","                _, super_predicted = torch.max(super_outputs.data, 1)\n","                _, sub_predicted = torch.max(sub_outputs.data, 1)\n","\n","                total += super_labels.size(0)\n","                super_correct += (super_predicted == super_labels).sum().item()\n","                sub_correct += (sub_predicted == sub_labels).sum().item()\n","                running_loss += loss.item()\n","\n","        print(f'Validation loss: {running_loss/i:.3f}')\n","        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n","        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n","\n","    def test(self, save_to_csv=False, return_predictions=False):\n","        if not self.test_loader:\n","            raise NotImplementedError('test_loader not specified')\n","\n","        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n","        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n","        with torch.no_grad():\n","            for i, data in enumerate(self.test_loader):\n","                inputs, img_name = data[0].to(device), data[1]\n","\n","                super_outputs, sub_outputs = self.model(inputs)\n","                _, super_predicted = torch.max(super_outputs.data, 1)\n","                _, sub_predicted = torch.max(sub_outputs.data, 1)\n","\n","                test_predictions['image'].append(img_name[0])\n","                test_predictions['superclass_index'].append(super_predicted.item())\n","                test_predictions['subclass_index'].append(sub_predicted.item())\n","\n","        test_predictions = pd.DataFrame(data=test_predictions)\n","\n","        if save_to_csv:\n","            test_predictions.to_csv('example_test_predictions.csv', index=False)\n","\n","        if return_predictions:\n","            return test_predictions"]},{"cell_type":"code","execution_count":null,"id":"ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1","metadata":{"id":"ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1"},"outputs":[],"source":["# Init model and trainer\n","device = 'cuda'\n","model = CNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"]},{"cell_type":"code","execution_count":null,"id":"7941c289-d9b1-4714-b788-898b3b889f58","metadata":{"id":"7941c289-d9b1-4714-b788-898b3b889f58","outputId":"af194fd2-f457-4b91-ef59-ca726f5a7565"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","Training loss: 4.238\n","Validation loss: 3.783\n","Validation superclass acc: 87.66 %\n","Validation subclass acc: 19.15 %\n","\n","Epoch 2\n","Training loss: 3.156\n","Validation loss: 3.244\n","Validation superclass acc: 88.61 %\n","Validation subclass acc: 29.11 %\n","\n","Epoch 3\n","Training loss: 2.588\n","Validation loss: 2.851\n","Validation superclass acc: 91.46 %\n","Validation subclass acc: 34.65 %\n","\n","Epoch 4\n","Training loss: 2.160\n","Validation loss: 2.543\n","Validation superclass acc: 89.56 %\n","Validation subclass acc: 38.92 %\n","\n","Epoch 5\n","Training loss: 1.780\n","Validation loss: 2.336\n","Validation superclass acc: 91.14 %\n","Validation subclass acc: 49.37 %\n","\n","Epoch 6\n","Training loss: 1.511\n","Validation loss: 2.361\n","Validation superclass acc: 90.66 %\n","Validation subclass acc: 46.52 %\n","\n","Epoch 7\n","Training loss: 1.214\n","Validation loss: 2.422\n","Validation superclass acc: 91.30 %\n","Validation subclass acc: 48.89 %\n","\n","Epoch 8\n","Training loss: 1.003\n","Validation loss: 2.194\n","Validation superclass acc: 91.14 %\n","Validation subclass acc: 49.84 %\n","\n","Epoch 9\n","Training loss: 0.778\n","Validation loss: 2.287\n","Validation superclass acc: 93.35 %\n","Validation subclass acc: 53.32 %\n","\n","Epoch 10\n","Training loss: 0.594\n","Validation loss: 2.452\n","Validation superclass acc: 93.51 %\n","Validation subclass acc: 53.96 %\n","\n","Epoch 11\n","Training loss: 0.464\n","Validation loss: 2.700\n","Validation superclass acc: 90.51 %\n","Validation subclass acc: 50.95 %\n","\n","Epoch 12\n","Training loss: 0.328\n","Validation loss: 2.546\n","Validation superclass acc: 93.35 %\n","Validation subclass acc: 53.48 %\n","\n","Epoch 13\n","Training loss: 0.201\n","Validation loss: 2.698\n","Validation superclass acc: 93.04 %\n","Validation subclass acc: 54.11 %\n","\n","Epoch 14\n","Training loss: 0.260\n","Validation loss: 2.660\n","Validation superclass acc: 92.09 %\n","Validation subclass acc: 55.38 %\n","\n","Epoch 15\n","Training loss: 0.226\n","Validation loss: 2.854\n","Validation superclass acc: 93.51 %\n","Validation subclass acc: 53.01 %\n","\n","Epoch 16\n","Training loss: 0.174\n","Validation loss: 3.001\n","Validation superclass acc: 92.25 %\n","Validation subclass acc: 55.54 %\n","\n","Epoch 17\n","Training loss: 0.147\n","Validation loss: 3.403\n","Validation superclass acc: 90.98 %\n","Validation subclass acc: 52.06 %\n","\n","Epoch 18\n","Training loss: 0.182\n","Validation loss: 3.230\n","Validation superclass acc: 93.04 %\n","Validation subclass acc: 53.01 %\n","\n","Epoch 19\n","Training loss: 0.111\n","Validation loss: 3.183\n","Validation superclass acc: 93.67 %\n","Validation subclass acc: 53.32 %\n","\n","Epoch 20\n","Training loss: 0.131\n","Validation loss: 3.503\n","Validation superclass acc: 90.82 %\n","Validation subclass acc: 51.42 %\n","\n","Finished Training\n"]}],"source":["# Training loop\n","for epoch in range(20):\n","    print(f'Epoch {epoch+1}')\n","    trainer.train_epoch()\n","    trainer.validate_epoch()\n","    print('')\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"id":"16d17e37-1a08-4ae1-8517-a16ff4769622","metadata":{"id":"16d17e37-1a08-4ae1-8517-a16ff4769622"},"outputs":[],"source":["trainer.test(save_to_csv=False, return_predictions=True)\n","\n","'''\n","This simple baseline scores the following test accuracy\n","\n","Superclass Accuracy\n","Overall: 43.83 %\n","Seen: 61.11 %\n","Unseen: 0.00 %\n","\n","Subclass Accuracy\n","Overall: 2.03 %\n","Seen: 9.56 %\n","Unseen: 0.00 %\n","'''"]},{"cell_type":"code","execution_count":null,"id":"6ab70fb9-6e14-49f1-b9bb-5f3da6807399","metadata":{"id":"6ab70fb9-6e14-49f1-b9bb-5f3da6807399"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[{"file_id":"1gNiSnCfVrjAQDLv_mHORzs0S2UZVOjst","timestamp":1702175149704}]}},"nbformat":4,"nbformat_minor":5}